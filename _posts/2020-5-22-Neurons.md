---
layout: post
title: Artificial Neurons
description: The building blocks of Artificial Neural Networks
image: assets/images/post_neurons/neuron_cropped.png
---
If you are not living under a rock you might have heard about [artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) (ANNs).

This post is the first of a series in which I will introduce to the basic maths and concepts of ANNs and machine learning, possibly reaching the more advanced topics of this fascinating discipline while keeping things simple and clear.

The starting episode is therefore devoted to the building blocks of any neural network: **the neurons**.

<br/>
The first type of artificial neuron, called ***perceptron*** was developed in the 60s by the scientist [Frank Rosenblatt](https://books.google.ca/books/about/Principles_of_neurodynamics.html?id=7FhRAAAAMAAJ&hl=en) inspired by earlier work by Warren McCulloch and Walter Pitts.
Nowadays it's more common to use other models of artificial neurons such as the *sigmoid neuron* whose properties will be described later on much more easily once we understand how perceptrons works.

Perceptrons are quite simple objects which takes **several binary inputs**, $x_1$, $x_2$, ..., and produce **one binary output**, $o$:

<p align="center">
  <img width="450px" src="/assets/images/post_neurons/neuron001_cropped.png">
  <figcaption class="caption-center"> Fig.1 </figcaption>
</p>

The simple approach proposed by Rosenblatt to compute the output was by introducing *weights*, $w_1$, $w_2$, ..., real numbers expressing the importance of the respective inputs to the output.
The neuron's output, 0 or 1, is determined by whether the weighted sum $\sum_{j} w_j x_j$  is less than or greater than some threshold value which, just like the weights, is another real number.

To put it in a more formal way:

$$
  \begin{equation}
    o =
    \begin{cases}
      0 & \text{if} \quad \sum_{j} w_j x_j \quad \le \quad \text{threshold}\\
      1 & \text{if} \quad \sum_{j} w_j x_j \quad \gt \quad \text{threshold}
    \end{cases}
  \end{equation}
$$

And that is basically everything we have to say from a mathematical point of view about perceptrons !
<br/>

We can be a little bit more fancy and simplify the notation by rewriting the weighted sum as a [dot product](https://en.wikipedia.org/wiki/Dot_product#Algebraic_definition), $\sum_{j} w_j x_j = w \cdot x$ where $w$ and $x$ are vectors whose components are the weights and inputs, respectively.
Also we can move the threshold on the other side of the inequality and replace it by what's known as the perceptron's ***bias***, $b \equiv - \text{threshold}$.

Eq (1) becomes:

$$
  \begin{equation}
    o =
    \begin{cases}
      0 & \text{if} \quad w \cdot x  + b \le 0\\
      1 & \text{if} \quad w \cdot x  + b \gt 0
    \end{cases}
  \end{equation}
$$

You can think of the bias as a measure of how easy it is to get the perceptron to output a 1. Or to put it in more biological terms, the bias is a measure of how easy it is to get the perceptron to *fire*. For a perceptron with a really big bias, it's extremely easy for the perceptron to output a 1. But if the bias is very negative, then it's difficult for the perceptron to output a 1. Obviously, introducing the bias is only a small change in how we describe perceptrons, but we'll see later that it leads to further notational simplifications.

To make a trivial example, let us consider the perceptron in Fig.1 with 3 inputs.
The output for a 3-inputs perceptron can be calculated using Eq.2 which becomes, explicitly:

$$
  \begin{equation}
    o \quad = \quad w \cdot x  + b \quad = \quad x_1 * w_1 + x_2 * w_2 + x_3 * w_3 - b
  \end{equation}
$$

where $ * $ is the classic multiplication operator.

Let's say that the output is whether you go ($o = 1$) or not ($o = 0$) to a music festival based on:

1. your favourite band is playing ($x_1 = 1$) or not ($x_1 = 0$)
2. your friends are coming ($x_2 = 1$) or not ($x_2 = 0$)
3. it is sunny ($x_3 = 1$) or not ($x_3 = 0$)

If the most important thing for you is the band playing and then your friends coming while you don't care about weather, the perceptron which best model this kind of decision-making would have for example $w_1 = 6$, $w_2 = 4$ and $w_3 = 0$. Finally, let's say that this perceptron has a threshold of 5 (bias, $b = -5$).
For this choice that means that you will go to the concert ($o = 1$) whenever your favourite band is playing and it actually doesn't matter if your friends are coming or if it is sunny (just use Eq. 3 to check it).

By varying the *weights* and *bias* we can get different models of decision-making. Let's say for example that the bias is $b = -4$ (i.e. you're more inclined to go to the the concert, in general): in this case you will go to the concert if your favourite band is playing *or* if your friend are coming (or both), regardless of the atmospheric conditions.

<p align="center">
  <img width="600px" src="/assets/images/post_neurons/2hiddenANN.png">
  <figcaption class="caption-center"> Fig.2    <i></i></figcaption>
</p>

<!-- {% include image.html url="assets/images/post_neurons/2hiddenANN.png" description="Yo" %} -->
<!-- image taken from this [post](https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6#106c) -->

**TO BE CONTINUED....**
