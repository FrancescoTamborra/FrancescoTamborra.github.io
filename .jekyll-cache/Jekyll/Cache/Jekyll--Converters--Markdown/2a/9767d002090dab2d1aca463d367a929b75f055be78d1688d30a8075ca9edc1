I"¾<p>If you are not living under a rock you might have heard about <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">artificial neural networks</a> (ANNs).</p>

<p>This post is the first of a series in which I will introduce to the basic maths and concepts of ANNs and machine learning, possibly reaching the more advanced topics of this fascinating discipline while keeping things simple and clear.</p>

<p>The starting episode is therefore devoted to the building blocks of any neural network: <strong>the neurons</strong>.</p>

<p><br />
The first type of artificial neuron, called <strong><em>perceptron</em></strong> was developed in the 60s by the scientist <a href="https://books.google.ca/books/about/Principles_of_neurodynamics.html?id=7FhRAAAAMAAJ&amp;hl=en">Frank Rosenblatt</a> inspired by earlier work by Warren McCulloch and Walter Pitts.
Nowadays itâ€™s more common to use other models of artificial neurons such as the <em>sigmoid neuron</em> whose properties will be described later on much more easily once we understand how perceptrons works.</p>

<p>Perceptrons are quite simple objects which takes <strong>several binary inputs</strong>, $x_1$, $x_2$, â€¦, and produce <strong>one binary output</strong>, $o$:</p>

<p align="center">
  <img width="450px" src="/assets/images/post_neurons/neuron001_cropped.png" />
  <figcaption class="caption-center"> Fig.1 </figcaption>
</p>

<p>The simple approach proposed by Rosenblatt to compute the output was by introducing <em>weights</em>, $w_1$, $w_2$, â€¦, real numbers expressing the importance of the respective inputs to the output.
The neuronâ€™s output, 0 or 1, is determined by whether the weighted sum $\sum_{j} w_j x_j$  is less than or greater than some threshold value which, just like the weights, is another real number.</p>

<p>To put it in a more formal way:</p>

\[\begin{equation}
    o =
    \begin{cases}
      0 &amp; \text{if} \quad \sum_{j} w_j x_j \quad \le \quad \text{threshold}\\
      1 &amp; \text{if} \quad \sum_{j} w_j x_j \quad \gt \quad \text{threshold}
    \end{cases}
  \end{equation}\]

<p>And that is basically everything we have to say from a mathematical point of view about perceptrons !
<br /></p>

<p>We can be a little bit more fancy and simplify the notation by rewriting the weighted sum as a <a href="https://en.wikipedia.org/wiki/Dot_product#Algebraic_definition">dot product</a>, $\sum_{j} w_j x_j = w \cdot x$ where $w$ and $x$ are vectors whose components are the weights and inputs, respectively.
Also we can move the threshold on the other side of the inequality and replace it by whatâ€™s known as the perceptronâ€™s <strong><em>bias</em></strong>, $b \equiv - \text{threshold}$.</p>

<p>Eq (1) becomes:</p>

\[\begin{equation}
    o =
    \begin{cases}
      0 &amp; \text{if} \quad w \cdot x  + b \le 0\\
      1 &amp; \text{if} \quad w \cdot x  + b \gt 0
    \end{cases}
  \end{equation}\]

<p>You can think of the bias as a measure of how easy it is to get the perceptron to output a 1. Or to put it in more biological terms, the bias is a measure of how easy it is to get the perceptron to <em>fire</em>. For a perceptron with a really big bias, itâ€™s extremely easy for the perceptron to output a 1. But if the bias is very negative, then itâ€™s difficult for the perceptron to output a 1. Obviously, introducing the bias is only a small change in how we describe perceptrons, but weâ€™ll see later that it leads to further notational simplifications.</p>

<p>To make a trivial example, let us consider the perceptron in Fig.1 with 3 inputs.
The output for a 3-inputs perceptron can be calculated using Eq.2 which becomes, explicitly:</p>

\[\begin{equation}
    o \quad = \quad w \cdot x  + b \quad = \quad x_1 * w_1 + x_2 * w_2 + x_3 * w_3 - b
  \end{equation}\]

<p>where $ * $ is the classic multiplication operator.</p>

<p>Letâ€™s say that the output is whether you go ($o = 1$) or not ($o = 0$) to a music festival based on:</p>

<ol>
  <li>your favourite band is playing ($x_1 = 1$) or not ($x_1 = 0$)</li>
  <li>your friends are coming ($x_2 = 1$) or not ($x_2 = 0$)</li>
  <li>it is sunny ($x_3 = 1$) or not ($x_3 = 0$)</li>
</ol>

<p>If the most important thing for you is the band playing and then your friends coming while you donâ€™t care about weather, the perceptron which best model this kind of decision-making would have for example $w_1 = 6$, $w_2 = 4$ and $w_3 = 0$. Finally, letâ€™s say that this perceptron has a threshold of 5 (bias, $b = -5$).
For this choice that means that you will go to the concert ($o = 1$) whenever your favourite band is playing and it actually doesnâ€™t matter if your friends are coming or if it is sunny (just use Eq. 3 to check it).</p>

<p>By varying the <em>weights</em> and <em>bias</em> we can get different models of decision-making. Letâ€™s say for example that the bias is $b = -4$ (i.e. youâ€™re more inclined to go to the the concert, in general): in this case you will go to the concert if your favourite band is playing <em>or</em> if your friend are coming (or both), regardless of the atmospheric conditions.</p>

<p align="center">
  <img width="600px" src="/assets/images/post_neurons/2hiddenANN.png" />
  <figcaption class="caption-center"> Fig.2    <i></i></figcaption>
</p>

<!-- <figure class="image">
  <img src="assets/images/post_neurons/2hiddenANN.png" alt="Yo">
  <figcaption>Yo</figcaption>
</figure>
 -->
<!-- image taken from this [post](https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6#106c) -->
:ET